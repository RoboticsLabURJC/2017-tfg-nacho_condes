\chapter{Evaluation}\label{sec:new_models}
The \gls{cnn} analyzed in Section~\ref{sec:understanding} is going to be taken as a starting point to build \emph{new models}. These models will be trained with different datasets and regularization methods and, finally, new architectures will be implemented. In this chapter, the tools developed in Chapter~\ref{ch:benchmark} will be employed to evaluate the results achieved by each of these \glspl{cnn}. These tests will serve as a way to study the effects of the learning process in the performance of these algorithms. Before talking about the \emph{performance} of the new models, the visualization of the convolutional layers filters and activation maps is going to be analyzed.

\section{Convolutional layers visualization}
The filters and activation maps discussed in this section belong to the convolutional layers of the \emph{\textit{0-1; Patience=2} model} that can be found in Section~\ref{subsec:early_stopping}. This model has been trained with the \textit{0-1} dataset (see Section~\ref{subsec:handmade}) and an early stopping rule with patience~2. Its architecture corresponds to the one defined in Section~\ref{sec:understanding}. In order to generate the activation maps, the model will be fed with a sample extracted from the \textit{0-1} dataset. This sample is shown in Figure~\ref{fig:sample}.
\begin{figure}
	\centering
	\includegraphics[width=0.3\linewidth, keepaspectratio]{figures/0-1_sample.png}
	\caption{Sample employed to generate the activation maps.}
	\label{fig:sample}
\end{figure}

\subsection{Filters}
When loading the weights of the \emph{first convolutional layer}, a Numpy array of shape~(1,~3,~3,~32) is obtained. This means that the weights are arranged in~32~filters of size~3x3. In this case, the input is a grayscale image, so the filters only have one channel (i.e. depth=1). Besides that, when examining their values, \emph{negative and positive coefficients} are found.

In Figure~\ref{fig:filters}, these filters are plotted. Some of the filters look too noisy to tell which kind of feature they are looking for. However, a few of them can be interpreted at first sight as follows:
\begin{itemize}
	\item \textbf{Horizontal edge-emphasizing filter}: filters~7,~9~and~23.
	\item \textbf{Vertical edge-emphasizing filter}: filters~8,~15~and~31.	
\end{itemize}

\begin{figure}
	\centering
	\includegraphics[width=0.92\linewidth, keepaspectratio]{figures/weights_conv2d_1.png}
	\caption{Filters of the first convolutional layer.}
	\label{fig:filters}
\end{figure}

The filters in the \emph{second convolutional layer} have been displayed as well. The weights in this layer are stored in a Numpy array of shape~(32,~3,~3,~32), which means that there are~32~filters with size~3x3~as well. However, this time their depth is~32, since there is one channel per activation map generated by the previous layer. As we get deeper into the \gls{cnn} and the dimensionality grows, the filters look noisier and become harder to interpret, as it is shown in Figure~\ref{fig:filters2}.
\begin{figure}
	\centering
	\includegraphics[width=1\linewidth, keepaspectratio]{figures/weights_conv2d_2_mod.png}
	\caption{Filters of the second convolutional layer.}
	\label{fig:filters2}
\end{figure}

\subsection{Activation maps}
Figure~\ref{fig:activation_maps} shows the activation maps that the \emph{first convolutional layer} of the model outputs. There are \emph{horizontal and vertical edge images} that confirm the interpretation of the filters given in the previous section. Besides that, some activation maps (2,~12,~19,~25~and~26) look \textit{dead}. If we look back into Figure~\ref{fig:filters}, these activation maps correspond to filters with \emph{almost flat coefficients}. This may be a signal of a high learning rate~\cite{cs231n}. In this case, the learning rate is not explicitly declared, because the ADADELTA optimizer uses an adaptive one.
\begin{figure}
	\centering
	\includegraphics[width=0.9\linewidth, keepaspectratio]{figures/activation_maps_conv2d_1.png}
	\caption{Activation maps of the first convolutional layer.}
	\label{fig:activation_maps}
\end{figure}

The activation maps of the \emph{second layer} are shown in Figure~\ref{fig:activation_maps2}. The images obtained look \emph{more specialized} than the ones in the previous layer. It's easier to tell to what kind of feature (e.g. edges and corners) each activation map is responding to.
\begin{figure}
	\centering
	\includegraphics[width=0.9\linewidth, keepaspectratio]{figures/activation_maps_conv2d_2.png}
	\caption{Activation maps of the second convolutional layer.}
	\label{fig:activation_maps2}
\end{figure}

It's important to note that values of the activation maps are \emph{always positive}, even when filters have negative coefficients. This is because the \gls{relu} activation function (see Equation~\ref{eq:relu}) turn all the negative values to zero.

\section{Augmented datasets}\label{sec:new_datasets}
The original model has been trained with each of the \emph{handmade datasets} described in Section~\ref{sec:datasets}. The number of epochs has been set to~12~and the evaluation has been carried out with the~\emph{1-6~test dataset}. The results that are shown in Table~\ref{tbl:datasets} lead to the following conclusions:
\begin{itemize}
	\item As it might be expected, results when training with the \emph{Sobel dataset} are much worse than the ones obtained with the other datasets, because we're testing with noisy images a \gls{cnn} trained with noiseless samples.
	\item The \emph{\textit{0-6} and \textit{1-6} models} are the ones that achieve better results, as they have been trained with much more samples than \textit{0-1} and \textit{1-1}.
	\item When \emph{comparing \textit{0-1} with \textit{1-1}, and \textit{0-6} with \textit{1-6}}, it can be seen that performance is almost the same. This means that the gradient image without noise and transformations is not adding much information to the model.
\end{itemize}
In Figure~\ref{fig:val_datasets}, the validation results obtained after every epoch when training the model with each dataset can be seen.
\begin{figure}
	\centering
	\includegraphics[width=1\linewidth, keepaspectratio]{figures/val_datasets.png}
	\caption{Validation results when training the model with different datasets.}
	\label{fig:val_datasets}
\end{figure}

Taking all this into account, it has been decided to keep working with the \emph{\textit{0-1} model}, which achieves a performance that is comparable with the other models with the advantage of a much lower computational cost.

\begin{table}
	\centering
	\begin{tabular}{l*{4}{c}r}
		\textbf{Model} & \textbf{Loss} & \textbf{Accuracy} & \textbf{Epochs} \\
		\hline
		Sobel & 1.233 & 0.699 & 12 \\
		0-1 & 0.201 & 0.939 & 12 \\
		1-1 & 0.189 & 0.943 & 12 \\
		0-6 & 0.109 & 0.968 & 12 \\
		1-6 & 0.111 & 0.967 & 12 \\
	\end{tabular}
	\caption{Results of training with different datasets.}
	\label{tbl:datasets}
\end{table}

\section{Regularization methods}
``\emph{Regularization} is any modification we make to a learning algorithm that is intended to reduce its \emph{generalization error} but not it's training error"~\cite{Goodfellow-et-al-2016}. Reducing the generalization error is important because, even if a model achieves a great accuracy or loss with the training dataset, if it doesn't generalize well enough, results during validation and test time won't be optimal. This is specially significant in our case, since the predictions of the digit classifier will be based on images that differ a lot from the training dataset. In this section, the effects of applying to the \emph{\textit{0-1} model} two regularization techniques (early stopping and dropout) are going to be evaluated. 

\subsection{Early stopping}\label{subsec:early_stopping}
The models in the previous section have been trained for~12~epochs. However, if we look at the \emph{validation results} in Figure~\ref{fig:val_datasets}, it can be assumed that the models were \emph{not overfitting} yet, because results didn't stop improving. This means that they were not being trained as much as possible. Setting an \emph{early stopping} rule (see Section~\ref{subsec:callbacks}) allows training the \gls{cnn} right until it starts to overfit, making the most of it. The criteria that has been used depends on the loss during validation. The model is trained until the \emph{log loss} (see Section~\ref{eq:categorical_crossentropy}) has not improved after two validations in a row, which means a \textit{patience} of 2. Besides that, in order to keep the best \textit{version} of the model, the log loss is checked after each epoch and, if the value is lower than the previous best log loss achieved, the weights of the model are saved, overwritting the weights of the previous best \textit{version}. The difference between training the model with and without early stopping can be seen in Table~\ref{tbl:earlystopping}.
\begin{table}
	\centering
	\begin{tabular}{l*{4}{c}r}
		\textbf{Model} & \textbf{Loss} & \textbf{Accuracy} & \textbf{Epochs} \\
		\hline
		0-1 & 0.201 & 0.939 & 12 \\
		0-1; Patience=2 & 0.155 & 0.954 & 30 \\
	\end{tabular}
	\caption{Results of training with and without early stopping.}
	\label{tbl:earlystopping}
\end{table}

Early stopping means an improvement of 1.6\% in accuracy and 4.6\% in log-loss. The model has been trained for~30~epochs and it reached its best \textit{version} at the 27$^{th}$ epoch. Setting a longer \textit{patience} has been considered, but it has been decided to apply it only to the best model obtained in Section~\ref{subsec:arch} to reduce the computational cost.

\subsection{Dropout}
The models that have already been evaluated insert \emph{dropout} (see Section~\ref{subsec:layers}) before every dense layer of the \gls{cnn} (0.25\% and 0.5\%, respectively). Dropout is usually applied just to fully-connected or \emph{dense layers}, because convolutional layers are less likely to overfit due to their architecture. In order to determine how dropout affects the performance of the \glspl{cnn}, the \emph{\textit{0-1; Patience=2} model}, defined in the previous section, has been trained with and without the mentioned dropout. The results can be seen in Table~\ref{tbl:dropout}. 
\begin{table}
	\centering
	\begin{tabular}{l*{4}{c}r}
		\textbf{Model} & \textbf{Loss} & \textbf{Accuracy} & \textbf{Epochs} \\
		\hline
		No dropout & 0.189 & 0.945 & 9 \\
		Dropout & 0.155 & 0.954 & 30 \\
	\end{tabular}
	\caption{Results of training with and without dropout.}
	\label{tbl:dropout}
\end{table}

Without dropout, the model has stopped training after~9~epochs. It has \emph{learned faster}, but it has started \emph{overfitting} earlier, resulting in worst results that the ones achieved by the model trained with dropout. This can be clearly seen in Figure~\ref{fig:comp_dropout}
\begin{figure}
	\centering
	\includegraphics[width=1\linewidth, keepaspectratio]{figures/comp_dropout.png}
	\caption{Validation results with and without dropout.}
	\label{fig:comp_dropout}
\end{figure}

Additionally, in Figure~\ref{fig:lc_dropout}, the learning curves of both models can be seen. It's worth looking into these plots to realize that validation results are better than training results when the model is trained with dropout. This may seem illogical, as the \gls{cnn} should always perform better with samples that it has already seen. However, it's important to remember that dropout only applies during training and, as it will \textit{switch-off} a lot of weights in the \gls{cnn}, much of its prediction power will be lost. During validation, there are no \textit{switched-off} weights, which allows the \gls{cnn} to make better predictions. Besides that, in the figure can be seen that the training results are better when the model is trained without dropout, while the validation results are better with dropout. This means that the model with dropout is \emph{generalizing} better than the one without dropout. 
\begin{figure}
	\begin{subfigure}{1\textwidth}
		\centering
		\includegraphics[width=1\linewidth]{figures/lc_nodropout.png}
		\caption{}
	\end{subfigure}
	\begin{subfigure}{1\textwidth}
		\centering
		\includegraphics[width=1\linewidth]{figures/lc_dropout.png}
		\caption{}
	\end{subfigure}
	\caption{Learning curves: (a) without dropout; (b) with dropout.}
	\label{fig:lc_dropout}
\end{figure}

\section{New architectures}\label{subsec:arch}
In order to check the influence of \emph{different architectures} in the performance of the \gls{cnn}, new models with a different number of convolutional layers have been trained and tested. The stopping rule used in these trainings is the one defined in Section~\ref{subsec:early_stopping} and dropout is also applied. The decision of adding \emph{pooling layers} to the models (see Section~\ref{subsec:layers}) has been taken to reduce computational cost. In the first attempt at training a model with~6~convolutional layers, the model with~2~convolutional layers and one \emph{MaxPooling layer} was triplicated. However, the first MaxPooling layer of the model was removed because the model ended up working with an empty image:~0x0~size.
\begin{table}
	\centering
	\begin{tabular}{l*{4}{c}r}
		\textbf{Model} & \textbf{Loss} & \textbf{Accuracy} & \textbf{Epochs} \\
		\hline
		1Conv+MaxPooling & 0.191 & 0.945 & 47 \\
		2Conv+MaxPooling & 0.155 & 0.954 & 30 \\
		3Conv+MaxPooling & 0.129 & 0.945 & 28 \\
		2Conv+MaxPooling+2Conv+MaxPooling & 0.092 & 0.970 & 27 \\
		4Conv+MaxPooling+2Conv+MaxPooling & 0.092 & 0.971 & 24 \\
	\end{tabular}
	\caption{Results of training models with different architectures.}
	\label{tbl:arch}
\end{table}

As it is shown in Table~\ref{tbl:arch}, the best results have been obtained with the models that contain~4~and~6~convolutional layers. Besides that, taking a look into the validation curves (see Figure~\ref{fig:comp_arch}), it can be assumed that when the number of layers is increased, the neural network tends to lead to better results with less epochs.

\begin{figure}
	\centering
	\includegraphics[width=1\linewidth, keepaspectratio]{figures/full_comparison.png}
	\caption{Validation results with different architectures.}
	\label{fig:comp_arch}
\end{figure}

The model with~6~layers has a slightly better accuracy, but a slightly worse loss, than the one with~4~layers. Considering that computational cost is higher when training the \textit{6Conv} model, the \emph{\textit{4Conv} model} seems to be the best bet. In order to make the most of it, it has been trained again but increasing the \emph{\textit{patience}} of the early stopping rule from~2~to~5. The results obtained with this new stopping rule can be seen in Table~\ref{tbl:arch_patience5}. These results imply that being more \textit{patient} during training can lead to a better performance, although in this case the improvement is not very significant.
\begin{table}
	\centering
	\begin{tabular}{l*{4}{c}r}
		\textbf{Model} & \textbf{Loss} & \textbf{Accuracy} & \textbf{Epochs} \\
		\hline
		4Conv; Patience=2 & 0.092 & 0.970 & 27 \\
		4Conv; Patience=5 & 0.082 & 0.973 & 37 \\
	\end{tabular}
	\caption{\textit{4Conv} model trained with different stopping rules.}
	\label{tbl:arch_patience5}
\end{table}

A visualization of the performance achieved by the best model built in this project (i.e. \emph{4Conv; Patience=5 model}) is shown in Figure~\ref{fig:best}. The Octave function discussed in Section~\ref{subsec:octave-func} has been employed to generate the plots displayed in this figure.

\begin{figure}
	\centering
	\begin{subfigure}{1\textwidth}
		\centering
		\includegraphics[width=1\linewidth]{figures/lc_42.png}
		\caption{}
	\end{subfigure}
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1\linewidth]{figures/pr_42.png}
		\caption{}
	\end{subfigure}%
	\begin{subfigure}{0.5\textwidth}
		\centering
		\includegraphics[width=1\linewidth]{figures/cm_42.png}
		\caption{}
	\end{subfigure}
	\caption{Performance of \textit{4Conv; Patience=5} model: (a) learning curves; (b) precision and recall; (c) confusion matrix.}
	\label{fig:best}
\end{figure}