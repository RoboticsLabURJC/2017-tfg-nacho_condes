\chapter{Infrastructure}

	This chapter is destined to a brief description of all the available hardware/software resources on which we will rely along the project.\\
 
\section{Hardware}
	\subsection{Sony EVI D100P camera}
	\label{sec:3_ptz}
		\begin{figure}[h]
			\centering
			\begin{subfigure}[h]{0.4\linewidth}
				\centering
				\includegraphics[width=1.6in]{images/ptz_front}
				\caption{Front side.}
			\end{subfigure}
			\qquad
			\begin{subfigure}[h]{0.4\linewidth}
				\centering
				\includegraphics[width=1.6in]{images/ptz_back}
				\caption{Back side.}
			\end{subfigure}
			\caption{Sony EVI D100P.}
			\label{fig:3_evi}
		\end{figure} 
		
		The first used hardware element is the Sony EVI D100P\footnote{\url{https://pro.sony/en_IN/products/ptz-network-cameras/evi-d100-d100p-pal-}}. It is a \emph{PTZ} cam (which stands for \emph{Pan Tilt Zoom}) which, originally thought and designed for videoconferences, is equipped with a bunch of precision servo motors. This allows it to be teleoperated, performing a soft and steady two-dimensional movement on demand:
		\begin{itemize}
			\item \emph{Pan:} horizontal movement. It can take values from $-100$ to $100$ degrees from the centered position. This movement can be performed at a certain speed, which can be set between $1$ and $24$.

			\item \emph{Tilt:} vertical movement. Its range goes from $-30$ to $30$, and the movement speed can be also varied between $1$ and $20$.
		\end{itemize}
		
		Something remarkable about this device is that it is bidirectional: we \textit{receive} images from its camera, and, at the same time, we \textit{send} it commands to move the motors.\\
		
		\begin{description}
			\item[Motors] 
			The low-level implementation of the movement commands is the \emph{VISCA} protocol, a proprietary solution from the manufacturer (Sony). It is received by the cam through a RS-232C (the traditional low-rate serial interface before USB spread), so we can connect it to a modern computer with a RS232-USB interface.
			
			However, the driver that controls this camera (\autoref{sec:3_evicam_driver}) does not offer support for a \emph{zoom} movement, but it is not very relevant for this application.\\
			
			\item[Video] 		As the video sensor is an analogue device, we need to convert its information to a digital format. We achieve this with a video capture device (\autoref{fig:3_dazzle}), which outputs digital video. This image flow is processed by a ROS driver (\autoref{sec:3_usb_cam}), that will be later explained.
			
			\begin{figure}[h]
				\centering
				\includegraphics[width=3in]{images/pinnacle_dazzle}
				\caption{Analogue-digital video converter (Pinnacle Dazzle).}
				\label{fig:3_dazzle}
			\end{figure}
			
			
		\end{description}

	
		We have to be careful on the movement commands (as it will be seen on \autoref{sec:follow_ptz}).\\

		This is the device we use on our experimental approach to the \emph{detection + robotic behavioral} node (\autoref{sec:follow_ptz}), where the only response is moving the camera.\\

	\subsection{Asus Xtion Pro Live}
		\label{sec:3_xtion}
		It is a RGBD (RGB + Depth) sensor, designed by Asus for interactive PC applications development purposes.

		\begin{figure}[h]
			\centering
			\includegraphics[width=0.4\linewidth]{images/xtion}
			\caption{Asus Xtion Pro Live. IR emitter (left), and RGB and IR lenses (right).}
			\label{fig:3_xtion}
		\end{figure}

		It counts on its left side with an IR (\emph{infrared}) light emitter, which radiates beams like a conventional light bulb (that's its function). On the right side, we can find two sensors:
		\begin{itemize}
			\item \emph{RGB sensor:} a regular digital camera, with a resolution up to 1280x1024 px.
			
			\item \emph{Depth sensor:} measures distance to objects by receiving the reflections of the IR beams that we have mentioned above. It maps, for each pixel, the distance to that reflection (in mm), stored as a 16-bit long value. Thus, we can obtain a depth image, with a resolution of 640x480 px (@ 30 fps).
		\end{itemize}

	We have used it as the visual source in the developed \emph{detection + robotic behavioral} node (\autoref{chap:followperson}).\\
		
	
	\subsection{Turtlebot 2 robot}

		\begin{figure}[h]
			\centering
			\begin{subfigure}[h]{0.4\linewidth}
				\includegraphics[width=1.9in]{images/real_turtlebot_1}
				\caption{Frontal view.}
				\label{fig:3_turtlebot_front}
			\end{subfigure}
			\begin{subfigure}[h]{0.4\linewidth}
				\includegraphics[width=2.2in]{images/real_turtlebot_2}
				\caption{Side view.}
				\label{fig:3_turtlebot_side}
			\end{subfigure}
			\caption{Turtlebot development kit.}
			\label{fig:3_turtlebot}
		\end{figure}
		
		It is a research robot, composed by a structure jointed to a Kobuki robot (mobile base)\footnote{\url{http://kobuki.yujinrobot.com/about2/}}. According to its technical specifications\footnote{\url{https://www.robotnik.es/web/wp-content/uploads/2014/04/TB_robot.pdf}}, it can reach speeds of $700 \ mm/s$ (on straight line), and $180\ deg/s$ (turning). Into the attached structure, we can find mounted an Asus Xtion sensor.\\
	
		The user has the capability of connecting each of these devices via USB to the laptop, and place it at the top platform of the robot. From there, the computer can run the algorithm and command the movements. Every component can be handled with the respective ROS driver (which will be described later).
		
		The Turtlebot platform has been our main actuation platform for the developed \emph{detection + robotic behavioral} application (\autoref{chap:followperson}).\\
			

\section{Python}
	According to the official definition from \cite{python}, Python is \textit{an interpreted, object-oriented, high-level programming language with dynamic semantics}. It was created in 1991 by Guido van Rossum. However, due to the increasing growth of \emph{Machine Learning} that happened the last two decades, it has become the most popular language for this purpose. As it focus on \emph{easiness}, its duck typing\footnote{This refers to Python guessing about your code, coming from the phrase \textit{''if it looks like a duck and sounds like a duck, chances are it's a duck."}} and its strong Object Orientation (everything can be treated as an object on this language) are a big ace up the sleeve in comparison to other languages and alternatives. In the programming point of view, it is a very interesting feature, as it facilitates features as sharing memory, abstract processes, and much more.
	
	And it is \textit{Open Source}, so it is always under community improvements, and there are a vast number of useful third party libraries, which often are pretty easily deployable onto your code.\\
	
	All these points make this language a really potential candidate for the applications to develop (and that's precisely the reason that explains its  growth on the software market).\\
	
	Python is an \emph{interpreted} language, which means that its sentences are projected on another program (the CPython interpreter, which executes them), and not directly by the processing hardware (CPU/GPU). This can be a handicap, as it makes the code execution much slower, in comparison with standard \emph{compiled} languages, which are run directly as processes, and grabbed by the computer hardware for its execution (as C, C++, Picky, etc.).\\
	
	For our target, we have used Python on its version $2.7$. Although it is a relatively old version of the language, it is necessary to mantain the compatibility with ROS Kinetic distribution  (\autoref{sec:3_ros}) bindings, which have not still taken the leap to the newest major version ($3.x$) on Python.\\
	


\section{ROS robotics framework}
	\label{sec:3_ros}
	\emph{ROS} (Robot Operating System) is \textit{an open-source, meta-operating system for your robot}, maintained by the \emph{OSRF} (Open Source Robotics Foundation) \cite{ros-intro}. It is a framework that provides a distributed, easily-scalable environment of \emph{nodes}. These nodes are programs which are independently on the computer (or distributed over a network), so they can perform individual tasks. However, they can communicate between themselves on a synchronous way (over \emph{services}, implementing a client-server role system between nodes), or on an asynchronous way, via \textit{topics}. These topics, which rely on a standard TCP/UDP communication between sockets via the \texttt{loopback} interface, are intended for an unidirectional, streaming communication, where a node can take roles: \emph{publisher} (if it is writing data inside the topic), or \emph{subscriber} (if it is reading the data that publishers are broadcasting into the topic). The data stream through the topic is not unrestricted. It must follow a ROS specific syntax, the \emph{Message} type, which is strictly defined for the communication purpose (geometry, sensoring, etc.).\\
	
	For our project we have used the 2016 \textit{LTS} (Long Term Support) version, called \textit{Kinetic Kame}\footnote{\url{http://wiki.ros.org/kinetic}}. This is the version bundled on the installation of JdeRobot\footnote{\url{https://jderobot.org/Installation}}, which offers a full compatibility with it.\\
	

	\begin{figure}[h]
		\begin{lstlisting}
...
import rospy
from std_msgs.msg import String
...
rospy.init_node('listener')              # Starting the node entity.
rospy.Subscriber('chatter', String)      # Instantiation of the topic subscriber.
rospy.spin()                             # 'Infinite loop' listening to the topic.
...
		\end{lstlisting}
		\caption{Simple stablishment of a listener node through \texttt{rospy} (code from \cite{listener-rospy}).}
		\label{fig:3_rospy_listener}
	\end{figure}
	
	ROS provides libraries and bindings for C++, Lisp, and \textit{Python} (\texttt{rospy}). They allow to really easily set up a topic between two or more programs, which will be seen as ROS nodes. However, this topic communication will be abstracted on our project by the \texttt{comm} library, as it will be seen on \autoref{sec:3_comm}.\\
	
	ROS also provides a Debian package, called \texttt{rosbash}, which allows to, in a very handy way, manage nodes and packages from a standard \texttt{bash} shell. The most remarkable feature for us is the command \texttt{roslaunch}, that launches a ROS node with a certain specific settings, configurable via a \texttt{.launch} file (which follows a XML formatting). An example for the file structure can be found on \autoref{fig:3_launch_file}.\\
	
	\subsection{usb\_cam driver}
	\label{sec:3_usb_cam}
		It is a ROS driver that creates a topic and publishes into it the digital video data incoming from a USB camera, into the topic \texttt{/usb\_cam/image\_raw}.\\
		
		This driver has been used on some experiments about the \emph{detection + robotic behavioral} application (\autoref{sec:follow_ptz}), with the purpose of retrieving images from the Sony EVI D100P camera (\autoref{fig:3_evi}). A custom configuration file\footnote{\url{https://github.com/RoboticsURJC-students/2017-tfg-nacho\_condes/blob/master/resources/usb\_cam-test.launch}} is required. We can have a glance on that configuration file on \autoref{fig:3_launch_file}.\\
		
		\begin{figure}[h]
			\centering
			\includegraphics[width=5in]{images/usb_cam_test}
			\caption{Example of \texttt{usb\_cam-test.launch} configuration file for a ROS node.}
			\label{fig:3_launch_file}
		\end{figure}
		\begin{center}
			Usage: \texttt{roslaunch usb\_cam-test.launch}
		\end{center}

	\subsection{openni2\_launch driver}
	
		The ROS binding at \cite{openni2-doc} provides the launch files for the \texttt{rgbd\_launch} node. This node publishes on several topics the RGB+D images provided by the Asus Xtion (\autoref{fig:3_xtion}).\\
		

		
		As it can be seen on \autoref{fig:3_xtion}, both sensors (RGB and infrared) can't physically be in the same place, so there is a little discrepancy between both computed image:
		
		\begin{figure}[h!]
			\centering
			\begin{subfigure}[h]{0.4\linewidth}
				\centering
				\includegraphics[width=2.7in]{images/rgb_before}
				\caption{RGB image.}
				\label{fig:3_rgb_bef_reg}
			\end{subfigure}
			\hfill
			\begin{subfigure}[h]{0.4\linewidth}
				\centering
				\includegraphics[width=2.7in]{images/depth_before}
				\caption{Depth image.}
				\label{fig:3_depth_bef_reg}
			\end{subfigure}
			
			\caption{Both raw (before registration) images sensed by the Xtion cameras.}
			\label{fig:3_bef_reg}
		\end{figure}
		
		With the goal of palliating this disparity, a process called \emph{registration} is executed for every new incoming depth image. It consists of a projection of the depth pixels into the RGB image, trying to align on an optimum way each depth pixel with its counterpart on the RGB image. We can observe that this can cancel to a certain point the difference between both images (\autoref{fig:3_disparities_comp}).
		
		\begin{figure}[h]
			\begin{subfigure}[h]{0.4\linewidth}
				\centering
				\includegraphics[width=2.7in]{images/disparity_before}
				\caption{Before registration.}
				\label{fig:3_disparity_bef_reg}
			\end{subfigure}
			\hfill
			\begin{subfigure}[h!]{0.4\linewidth}
				\centering
				\includegraphics[width=2.7in]{images/disparity_after}
				\caption{After registration.}
				\label{fig:3_disparity_aft_reg}
			\end{subfigure}
			\caption{Comparison between the disparities, before and after the registration process.}
			\label{fig:3_disparities_comp}
		\end{figure}
		
		If we compare the new disparity (\autoref{fig:3_disparity_aft_reg}) with the previous one (\autoref{fig:3_disparity_bef_reg}), we can realize that now the RGB and Depth images are aligned on an improved way, as if both sensors were on the same place, or much closer at least. So, from now on, we will call \emph{depth image} to the registered version of the depth map, as the unregistered image is not useful anymore.\\
		
		The open source driver working behind this binding is called OpenNI\footnote{\url{https://structure.io/openni}} (\emph{Open Natural Interaction}). It was originally written by the Kinect developer company PrimeSense (which designed the Xtion device beside Asus).\\
		
		In summary, this interface allows to perform both processes involved into handling this device (image grabbing and depth registration). It has been used on the {detection + robotic behavioral} application (\autoref{chap:followperson}). It will connect the Xtion sensor to it, providing the real time imaging through the corresponding topic.\\
		
		\begin{center}
			Usage: \texttt{roslaunch openni2\_launch openni2.launch}
		\end{center}

	\subsection{kobuki\_node package}
		This ROS package contains a bunch of launch files. Among them there is the one we have used: \texttt{minimal.launch}, which starts the \emph{nodelet}\footnote{A ROS nodelet performs multiple simultaneous processes, and consequently opens several topics.} that gives us the total control of the Turtlebot2 robot connected to the computer.\\
		
		This node will be used on the second iteration of the \emph{detection + robotic behavioral} application (\autoref{chap:followperson}). It will connect the Turtlebot motors to the component, providing the topic to command movements to them.
		
		\begin{center}
			Usage: \texttt{roslaunch kobuki\_node minimal.launch}
		\end{center}

	
\section{JdeRobot robotics framework}
	As described in \autoref{sec:dl_jderobot}, JdeRobot\footnote{\url{https://jderobot.org}} is a distributed development middleware, born in \cite{jmplaza-phd}. It stands out mainly for two key aspects:
	\begin{itemize}
		\item \textit{Hardware abstraction:} it behaves as an intermediate layer between control software (written by the programmer) and hardware, which can be a real device (a robot, drone, camera, laser scanner, etc.), or a simulated device (on the open source world simulator Gazebo\footnote{\url{http://gazebosim.org/}}). The bidirectional flow (information from sensors, and commands from the computer) is sent the same way, \textit{no matter the kind of the underlying robotic device}.
		
		As well, this abstraction layer allows various computers to interact simultaneously with the hardware, as the communications are also abstracted to ROS topics or ICE endpoints (it will be properly explained at \autoref{sec:3_comm}), where a program has to just listen/talk. This provides \textit{software and hardware scalability to the platform, and to the developed programs}.\\
		
		Let's have a look on a possible example on the \autoref{fig:3_jderobot_hal}. This could represent an scenario where somebody wants to virtually test a navigation algorithm. Thus, in the \emph{Computer 1}, a reactive controller is running, sensing the environment through a real laser scanner and a RGB camera (as in the work developed at \cite{rocapal}). This controller receives data from the sensors, computes a proper navigation response, and sends it to a virtual robot, simulated on Gazebo.
		
		Additionally, another machine (\emph{Computer 2}) is running a viewer, which allows it to draw the images seen by the camera, and the laser readings from the scanner. So, this component only receives the data from the sensors, and does not send any kind of data to the devices.
		
		We can see that both components can perfectly run together and on different machines, even when they are written over completely different languages (Python and C++, respectively). In addition, we can perfectly handle virtual and real devices simultaneously, even if they talk through different interfaces (ROS or ICE), due to the perfect support to both by the \texttt{comm}  library (\autoref{sec:3_comm}). 		 \textit{In this easiness and flexibility resides the main advantage of using JdeRobot}\\
		
		
		\begin{figure}[h]
			\centering
			\includegraphics[width=4in]{images/jderobot_hal}
			\caption{JdeRobot abstraction layer, and a possible use distributed, multi-middleware scenario.}
			\label{fig:3_jderobot_hal}
		\end{figure}
		
.
		
		\item \textit{Wide device support:} JdeRobot provides full compatibility with ROS Kinetic Kame, so it can perfectly integrate ROS Nodes (in our concern, we can communicate with the Turtlebot and the Xtion devices via several topics that the ROS intermediate nodes open).
		
		\item \textit{Threaded software architecture for robotics applications:} as it is introduced at  \cite{jmplaza-phd}, inside a component, we will find one or more threads. These threads run concurrently with an specific timing (so it does not overload the CPU in vain if a few iterations per second are enough for a vivacious and correct response).\\
		
		These schemes perform different tasks each, on a non-blocking way, and share memory. This has been followed on a comfortable way on our implementation: the threads are independent, but the tasks they control are performed by Python objects, which are interconnected between them:
		
		
	\end{itemize}
	
	Now, in the next subsections, we will examine which of the available JdeRobot components, apart of the infrastructure, have been of greatest interest for us.
	
	\subsection{Digit Classifier node}
	\label{sec:3_digitclassifier_jderobot}
		This JdeRobot component was originally designed by David Pascual \cite{dpascualhe} and Nuria Oyaga \cite{noyaga}, and it was used on this project to land on the field of neural networks.\\
		\begin{figure}[h]
			\centering
			\includegraphics[width=4in]{images/digitclassifier}
			\caption{\texttt{DigitClassifier} on action.}
			\label{fig:3_digitclassifier}
		\end{figure}
		
		
		In rough outline, its function is to classify on-demand or in real-time the incoming images from a video source, mapping them into digits from $0$ to $9$. There are two identical version, differentiated on the underlying framework (Keras or Caffe). On \autoref{fig:3_digitclassifier} we can see its operation: it processes an image extracting the shown digit and showing the class that the network has assigned to it.\\
	
	
	\subsection{evicam\_driver driver}
	\label{sec:3_evicam_driver}
	This driver, bundled into JdeRobot\footnote{\url{https://github.com/JdeRobot/JdeRobot/tree/master/src/drivers/evicam\_driver}}, allows the user to send movements commands to a Sony EVI D100P camera \autoref{fig:3_evi}) and to retrieve information from it, creating an ICE endpoint that is ready to interact with the camera \emph{PT} (Pan, Tilt) motors.\\
	
	As this is a low-level driver, written in C++, it requires to be used on a specific way, which has been documented\footnote{\url{https://jderobot.org/Handbook\#PanTilt_Teleop}} to be easily applied in the future. This driver defines an interaction API with the camera, which allows us to get the values from the  encoders:
	\begin{lstlisting}
import config
import comm
...

cfg = config.load('yml_configuration_file')
jdrc = comm.init(cfg, 'NodeName')


# Instantiation for the motors:
PTMotors = jdrc.getPTMotorsClient('NodeName.PTMotorsEndpoint')

print(PTMotors.getLimits()) # Shows the max/min values for pan, tilt 
                            # and each speed.

print(PTMotors.motors.data) # Shows the current values for pan, tilt
                            # and each speed.

# Let's move the camera! As easy as:
PTMotors.setPTMotorsData(new_pan, new_tilt, max_pan_speed, max_tilt_speed)
	\end{lstlisting}
	
	\subsection{comm library}
		\label{sec:3_comm}
		\texttt{comm} is the basic library included on JdeRobot to perform communications between different components. It supports all the data flows in a typical scenario (\autoref{fig:3_jderobot_hal}).\\
		
		\texttt{comm} consists on a collection of bindings to easily create a link between two components, or between a device and a component. On the lowest level, we can use it relying on ROS (through topics as it was explained before on \autoref{sec:3_ros}), or through an ICE proxy. ICE\footnote{\url{https://zeroc.com/products/ice}} is an object-oriented middleware that, in our purpose, allows to abstract a data flow to a TCP/IP endpoint (an address/hostname, and a port), which can even support a communication between two or more programs inside the same machine.\\
		
		To create a communicator with \texttt{comm}, it needs the specification for that link (underlying middleware, topic/endpoint, etc.), so it uses the JdeRobot standard: YML\footnote{Legible data serialization format.} configuration files, which must follow a similar format to  \autoref{fig:3_yml_format}.
		\begin{figure}[h]
			\centering
			\includegraphics[width=4in]{images/yml_format}
			\caption{YML format required by \texttt{comm}.}
			\label{fig:3_yml_format}
		\end{figure}
		
		In the previous example (\ref{sec:3_evicam_driver}) we can see an example of an instantiation of a global communicator through \texttt{comm} (which then provides the clients to interact with the device).
		
\section{OpenCV library}
	OpenCV (\emph{Open Source Computer Vision}) is a C++/Python/Java open-source library\footnote{\url{https://opencv.org/}} (natively written in C++) for Computer Vision purposes. Among the classic/\emph{state-of-the-art} methods it bundles, we can find functions suitable for face recognition, image stitching, eye movements following, establishing markers for augmented reality, etc.\\
	
	Its general focus is \emph{efficiency and real-time functionality}, thank to low-level optimizations on the system hardware (i.e. integration with Nvidia CUDA and OpenCL GPU processing libraries). Thus, the excellent performance achieved by this open source library has turned it into the \emph{de facto} standard between every kind of users (from researchers to big companies or even governmental bodies, as their website stands).\\
	
	The main benefit we have grabbed from this library (on its version 3.3.1) has been mainly for image analysis (such as \emph{Haar Cascade} classifiers, or edge detectors) or transformations (color conversions, gaussian blurrings, etc.).\\

\section{NumPy library}
	NumPy\footnote{\url{http://www.numpy.org/}} (\emph{Numeric Python}) is a library for Python (written in C++), born to extend the numerical abilities of this language. It provides a powerful \texttt{array} class, which allows to keep a N-dimensional collection of values/objects in a really handy way (in comparison with Python's standard \emph{lists}). It also provides a rich interface to describe the arrays (such as advanced indexing, shaping, data formatting, etc.).\\
	This will such an useful resource on this work, for 3 main reasons:
	\begin{itemize}
		\item \emph{Matricial representation of images:} every processed image is handled as matrices or bigger order tensors (the concept of matrix generalized for any number of dimensions), so visualizing/slicing them becomes a trivial task.
		\item \emph{Abstract structure to keep objects:} it allows to store different objects in a \texttt{np.array} object, providing an advanced API for indexing, and conditional checks to instantly retrieve the elements fulfilling a specific condition.
		\item \emph{Saving variables into disk:} this is an useful feature for debugging purposes. \texttt{np.save()} allows to save any variable (even non-NumPy ones, like dictionaries), finding it on a \texttt{.npy} file, ready to be traced and debugged.
	\end{itemize}

In the same way than OpenCV, this is a numerical library widely adopted between Python users. This is due to the easiness of handling of its types and structures, that provides an immediate data exchange format with other parties software. It has made of it our main numerical engine on the developed purposes, having used it on its version 1.14.5.



\section{TensorFlow framework}
	\label{sec:3_tensorflow}
	TensorFlow\footnote{\url{https://www.tensorflow.org/}}, which has been the core component of this project, is an open-source software framework for high performance numerical computation. It was originally created by the Google Brain team, and it offers an excellent background for \emph{machine learning} tasks.\\
	
	Its internal functionality is based on \emph{graphs}, composed by nodes which operate and exchange data values establishing \emph{a flow of tensors} (as previously described, a tensor is the general term to describe a multidimensional structure). These tensors are handled in the backend of TensorFlow, so performing operations with tensors is really fast, in comparison with high-level mathematical libraries (NumPy). A tensor can be formed of different data types (images, words, poses, numbers, etc.), which is the key for the versatility it offers for a large variety of projects\footnote{\url{https://github.com/jtoy/awesome-tensorflow}}.\\
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=4in]{images/tf_graph}
		\caption{Basic graph on TensorFlow (2 convolutional layers fed to a cost function).}
		\label{fig:3_tf_graph}
	\end{figure}
	
	All these possibilities make TensorFlow a very optimized ecosystem to implement \emph{deep learning} models (Deep Neural Networks). In addition, it is optimized for parallel GPU hardware. This gives a network the opportunity to experiment a performance boost, since it can reduce significantly the time it takes to make an inference (and even work on a system with a cluster of GPUs, although this is focused to more exigent systems than the one we create on this work). For our\\
	
	Since it was launched (November 2015), it has been adopted by many big companies which have used TensorFlow as the base for their Artificial Intelligence applications, such as Twitter, Intel, Google, eBay, Xiaomi, Nvidia, etc.\\
	
	As we will see later in the specific components, it allows to \emph{train} a neural network (for its later use) or even \emph{load} a specific model (kept on a Google Protobuf\footnote{Google's open source mechanism to serialize structured data.} \texttt{.pb} file). This is a really interesting feature, given that we are able to retrieve a bunch of pretrained models and embed them into a generic neural network.\\
	
	The version we have been using along the project is the last available one nowadays (1.9.0), compiled from sources on a Nvidia GPU (through CUDA on its 9.2.88 version), in order to squeeze the maximum inference speed possible.\\
	
	We have also also taken advantage of an included tool, \emph{TensorBoard}, which allows to \emph{visualize} interesting contents about a neural network, from a log trace of its execution. It is possible to visualize the \emph{graph} (including nodes, tensor shapes, operations, etc.). Also, it provides a functionality to analyze the obtained weights for each layer, on advanced analysis techniques, as PCA (\emph{Principal Component Analysis}). We have used it to visualize all the networks we design or import.




\section{Keras framework}
	As it is stated in \cite{dpascualhe}, Keras is a high-level \emph{neural network framework}, written in Python and capable of running on top of either TensorFlow or Theano (another \emph{deep learning} library).\\
	
	Hence, it is an abstraction with the goal of programming and handling a neural network on an simpler way, relying on a powerful library (treated as \emph{backend}) as TensorFlow to perform all the numeric operations. As well as TensorFlow, it is capable of loading previously compiled and saved models, on the serialization standard HDF5\footnote{Hierarchical Data Format (v.5): general purpose format to store and manage data.} (\texttt{.h5} files).\\
	
	In our project, support has been provided to use this framework (selecting it on the YML configuration file) on its version 2.2.0, although our main interest has been TensorFlow due to the significative difference of processing speed between both frameworks (being TensorFlow twice as fast as Keras).
\section{PyQt framework}
	Qt\footnote{\url{https://www.qt.io/}} is an cross-platform object-oriented framework for building GUIs (\emph{Graphical User Interfaces}), originally developed by a Nokia department. It is distributed under a commercial license, although it has a standard GPL license for open-source projects.\\
	
	A third party company (RiverBank Computing) developed PyQt, a set of Python bindings to interact with Qt (originally written in C++). It is structured in units called \emph{Widgets}, which contain blocks (\emph{Labels}).\\
	
	All this allows to easily deploy a GUI-based(\emph{Graphic User Interface}) program:
	\begin{figure}[h]
		\centering
		\begin{subfigure}[h]{0.55\linewidth}
			\centering
			\begin{lstlisting}
import sys
from PyQt5 import QtGui, QtWidgets

def window():
	app = QtWidgets.QApplication(sys.argv)
	w = QtWidgets.QWidget()
	b = QtWidgets.QLabel(w)
	b.setText("Hello World!")
	w.setGeometry(100,100,200,50)
	b.move(50,20)
	w.setWindowTitle("PyQt")
	w.show()
	sys.exit(app.exec_())

if __name__ == '__main__':
	window()
			\end{lstlisting}
			\caption{\emph{Hello world} example code.}
		\end{subfigure}
		\qquad
		\begin{subfigure}[h]{0.35\linewidth}
			\centering
			\includegraphics[width=3in]{images/pyqt_helloworld}
			\caption{Resulting window.}
		\end{subfigure}
		\caption{Example of a \emph{Hello World} window with PyQt5 bindings.}
		\label{fig:3_pyqt_helloworld}
	\end{figure}
	
	As it is the last available version at the time this is developed, we have used the version 5 of Qt. Hence, our binding library is \texttt{PyQt5}, which has been used to build a live-capable GUI on an intuitive way, in all the developed tools.\\
	
\section{threading library}
	\label{sec:3_threading}
	\texttt{Threading}\footnote{\url{https://docs.python.org/2/library/threading.html}} is a Python standard library which offer a high-level API for threading processes. This means to run programs on more than one single job for the CPU, which allows to be capable of perform several tasks on a simultaneous way.\\
	
	This is very convenient for our purpose, as we want to stick to a multiprocessing paradigm. So, this way we can have dedicated threads to grab the new camera images, update the GUI, and make the Neural Network to load new inferences on the last image detected.\\
	
	The \texttt{threading} provides a generic class for a thread. Our only task is to create a custom class which inherits it, customizing the \texttt{\_\_init\_\_} and \texttt{run()} methods our own way:
	

	\begin{lstlisting}
...
import threading
from datetime import datetime
...

class MyThread(threading.Thread):

	def __init__(self, foo, bar):
		'''
		This is the method which will be called at the creation of the thread.
		'''
		self.my_foo = foo
		self.my_bar = bar
		self.time_cycle = 100 # ms
		threading.Thread.__init__(self)  # Rest of the initialization.
		
		
	def run(self):
		'''
		This is the task the thread will perform once.
		If we put an infinite loop inside, we have a periodic thread.
		'''
		while True:
			start_time = datetime.now()
			# Grab an image, or run an inference on the neural network...
			self.my_foo.doMyStuff(self.my_bar)
			end_time = datetime.now()
			dt = end_time - start_time
			
			# If it did not take the refresh time, it sleeps until it arrives.
			if dt < self.t_cycle:
				sleep(self.t_cycle - dt)
				
	\end{lstlisting}
	
In addition, as we can see, it is possible to control the update period of the thread, so we can decide how much time it will be elapsed between two consecutive executions of the thread task (and of course this is a tunable parameter).\\

The version we have used is the standard one included with the Python installation.
