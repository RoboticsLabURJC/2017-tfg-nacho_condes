\documentclass[11pt, a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage{url}


\title{Tracking and following a person using Deep Learning on RGBD sensors}
\author{Ignacio Condés \\
		Universidad Rey Juan Carlos \\
		\texttt{ignacio.condes.m@gmail.com} \and José María Cañas \\
		Universidad Rey Juan Carlos\\
		\texttt{jmplaza@gsyc.es}}


\begin{document}
	\maketitle
	
	\begin{abstract}
		During the last decades, robotics have been evolving hand in hand with \emph{machine learning} techniques \cite{speech-pattern-recognition} \cite{slam-feature-extraction}. All of these techniques share their fundamental need of a heavily complex preprocessing of the input, to perform a \emph{feature extraction}, specially on the \emph{Computer Vision}  field \cite[pp.~30--38]{slam-feature-extraction}. These features are the descriptors subsequently fed to the response algorithm.\\
		
		But, what if we make \emph{deep learning} be a part of this intelligence system? It would yield us an immediate benefit, as we would not need a heavy feature transformation/extraction algorithms if a deep neural network could \emph{learn to do it itself on the optimum way}.\\
		
		This work proposes to address person detection with a deep neural network, gathering  a RGBD sensor image. This valuable and confident information allow to track a single person, thanks to carrying out a facial identification process, and a spatial distance estimation with the depth information from the RGBD sensor.\\
		
		The proposed system has been successfully deployed on a real Turtlebot robot, drawing upon an Asus Xtion conventional RGBD camera, and a real-time detection CNN (\emph{Convolutional Neural Network}) implemented on TensorFlow.
	\end{abstract}
	
	
	
	

\bibliographystyle{unsrt}	
\bibliography{bibliography}
	
	
	
\end{document}